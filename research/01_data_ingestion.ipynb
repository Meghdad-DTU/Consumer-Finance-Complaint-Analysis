{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paladin/Downloads/Consumer-Finance-Complaint-Analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    from_date: str\n",
    "    to_date: str    \n",
    "    feature_store_dir: Path \n",
    "    downloaded_dir: Path\n",
    "    failed_downloaded_dir: Path            \n",
    "    metadata_file_path: Path\n",
    "    min_start_date: str\n",
    "    file_name: str    \n",
    "    datasource_url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from financeComplaint.constants import *\n",
    "from financeComplaint.utils import read_yaml_file, create_directories\n",
    "from financeComplaint.entity.metadata_entity import DataIngestionMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,                 \n",
    "                 params_filepath=PARAMS_FILE_PATH,\n",
    "                 saved_modelpath=SAVED_MODEL_PATH,\n",
    "                 ):\n",
    "       \n",
    "        self.config = read_yaml_file(config_filepath)\n",
    "        self.params = read_yaml_file(params_filepath)\n",
    "        self.saved_modelpath = saved_modelpath\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        self.timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S') \n",
    "        \n",
    "    \n",
    "    def get_data_ingestion_config(self, from_date: str=None, to_date: str=None) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        from date can not be less than min start date\n",
    "\n",
    "        if to_date is not provided automatically current date will become to date\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        config = self.config.data_ingestion\n",
    "        SUB_ROOT_DIR = os.path.join(config.ROOT_DIR, self.timestamp)\n",
    "        DOWNLOADED_DIR = os.path.join(SUB_ROOT_DIR,'downloaded_files')\n",
    "        FAILED_DOWNLOADED_DIR = os.path.join(SUB_ROOT_DIR,'failed_downloaded_files')\n",
    "        \n",
    "\n",
    "        create_directories([config.ROOT_DIR, \n",
    "                            config.FEATURE_STORE_DIR,\n",
    "                            DOWNLOADED_DIR, \n",
    "                            FAILED_DOWNLOADED_DIR, \n",
    "                            ])\n",
    "    \n",
    "        def validate(date_text):\n",
    "            try:\n",
    "                if date_text != datetime.strptime(date_text, \"%Y-%m-%d\").strftime('%Y-%m-%d'):\n",
    "                    raise ValueError\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "        \n",
    "          \n",
    "        min_start_date= config.MIN_START_DATE \n",
    "        if  not validate(min_start_date):\n",
    "            raise Exception(f\"WARNING: Minimum start date: {min_start_date} does not have correct format!\")    \n",
    "        \n",
    "        if from_date is None:\n",
    "            from_date = min_start_date \n",
    "        else:\n",
    "            if not validate(from_date):\n",
    "                raise Exception(f\"WARNING: From date: {from_date} does not have correct format!\")\n",
    "        \n",
    "        if from_date < min_start_date:\n",
    "            from_date = min_start_date\n",
    "        \n",
    "        if to_date is None:\n",
    "            to_date = datetime.now().strftime(\"%Y-%m-%d\")  \n",
    "        \n",
    "        else:\n",
    "            if not validate(to_date):\n",
    "                raise Exception(f\"WARNING: To date : {to_date} does not have correct format!\") \n",
    "\n",
    "        data_ingestion_metadata= DataIngestionMetadata(config.METADATA_FILE_PATH)\n",
    "\n",
    "        if data_ingestion_metadata.is_metadata_file_exist:\n",
    "            metadata_info= data_ingestion_metadata.get_metadata_info()\n",
    "            from_date = metadata_info.to_date\n",
    "        \n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir = config.ROOT_DIR,\n",
    "            from_date= from_date,\n",
    "            to_date= to_date,\n",
    "            feature_store_dir= config.FEATURE_STORE_DIR,   \n",
    "            downloaded_dir = DOWNLOADED_DIR,\n",
    "            failed_downloaded_dir= FAILED_DOWNLOADED_DIR,                     \n",
    "            metadata_file_path= config.METADATA_FILE_PATH,\n",
    "            min_start_date= config.MIN_START_DATE,  \n",
    "            file_name= config.FILE_NAME,         \n",
    "            datasource_url= config.DATASOURCE_URL, \n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/11 14:53:24 WARN Utils: Your hostname, ds-xps resolves to a loopback address: 127.0.1.1; using 192.168.2.16 instead (on interface wlp2s0)\n",
      "23/10/11 14:53:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/paladin/Downloads/Consumer-Finance-Complaint-Analysis/venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/paladin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/paladin/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7675053c-9b45-4c9f-81a0-7d51b4f5080b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2 in central\n",
      "\tfound commons-codec#commons-codec;1.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.1.1 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\t[2.12.5] joda-time#joda-time;[2.2,)\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.3 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.3 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      ":: resolution report :: resolve 3211ms :: artifacts dl 51ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.3 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2 by [org.apache.httpcomponents#httpclient;4.2.5] in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2 by [org.apache.httpcomponents#httpcore;4.2.5] in [default]\n",
      "\tcommons-codec#commons-codec;1.6 by [commons-codec#commons-codec;1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.3 by [commons-codec#commons-codec;1.4] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.1.1 by [com.fasterxml.jackson.core#jackson-core;2.2.3] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.1.1 by [com.fasterxml.jackson.core#jackson-databind;2.2.3] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.1.1 by [com.fasterxml.jackson.core#jackson-annotations;2.2.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   76  |   1   |   0   |   8   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7675053c-9b45-4c9f-81a0-7d51b4f5080b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/25ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/11 14:53:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from financeComplaint.logger import logging\n",
    "from financeComplaint.exception import CustomException\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from financeComplaint.entity.artifact_entity import DataIngestionArtifact\n",
    "from financeComplaint.entity.metadata_entity import DataIngestionMetadata\n",
    "from financeComplaint.pipeline.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen= True)\n",
    "class DownloadURL:\n",
    "    url: str\n",
    "    file_path: Path\n",
    "    n_retry: int\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "    # Used to download data in chunks\n",
    "    def __init__(self, config: DataIngestionConfig, n_retry: int=5):\n",
    "        \"\"\"\n",
    "        config: Data Ingestion configuration\n",
    "        n_retry: number of retry failed should be tried to download  in case of failure encountered        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.config = config\n",
    "            self.failed_download_urls: List[DownloadURL] = []\n",
    "            self.n_retry = n_retry\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def get_required_interval(self):\n",
    "        try:\n",
    "            start_date = datetime.strptime(self.config.from_date, \"%Y-%m-%d\")\n",
    "            end_date = datetime.strptime(self.config.to_date, \"%Y-%m-%d\")\n",
    "            n_diff_days = (end_date - start_date).days\n",
    "            freq = None\n",
    "            if n_diff_days > 365:\n",
    "                freq = \"Y\"\n",
    "            elif n_diff_days > 30:\n",
    "                freq = \"M\"\n",
    "            elif n_diff_days > 7:\n",
    "                freq = \"W\"\n",
    "            logging.debug(f\"{n_diff_days} hence freq: {freq}\")\n",
    "            if freq is None:\n",
    "                intervals = pd.date_range(start=self.config.from_date,\n",
    "                                        end=self.config.to_date,\n",
    "                                        periods=2).astype('str').tolist()\n",
    "                \n",
    "            else:\n",
    "                intervals = pd.date_range(start=self.config.from_date,\n",
    "                                        end=self.config.to_date,\n",
    "                                        freq=freq).astype('str').tolist()\n",
    "\n",
    "            logging.debug(f\"Prepared Interval: {intervals}\")\n",
    "            if self.config.to_date not in intervals:\n",
    "                intervals.append(self.config.to_date)\n",
    "            return intervals\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def download_files(self, n_day_interval_url: int = None):\n",
    "        try:\n",
    "            required_interval = self.get_required_interval()\n",
    "            logging.info(\"Started downloading files\")\n",
    "            for index in range(1, len(required_interval)):\n",
    "                from_date, to_date = required_interval[index - 1], required_interval[index]\n",
    "                logging.debug(f\"Generating data download url between {from_date} and {to_date}\")\n",
    "                datasource_url: str = self.config.datasource_url\n",
    "                url = datasource_url.replace(\"[to_date]\", to_date).replace(\"[from_date]\", from_date)\n",
    "                file_name = f\"{self.config.file_name}_{from_date}_{to_date}.json\"\n",
    "                logging.debug(f\"Url: {url}\")\n",
    "                file_name = f\"{self.config.file_name}_{from_date}_{to_date}.json\"\n",
    "                file_path = os.path.join(self.config.downloaded_dir, file_name)\n",
    "                download_url = DownloadURL(url=url, file_path=file_path, n_retry=self.n_retry)   \n",
    "                self.download_data(download_url=download_url)         \n",
    "            logging.info(f\"File download completed!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def download_data(self, download_url: DownloadURL):\n",
    "        try:\n",
    "            logging.info(f\"Starting download operation: {download_url}\")\n",
    "            download_dir = os.path.dirname(download_url.file_path)\n",
    "\n",
    "            # creating download directory\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "            # downloading data\n",
    "            data = requests.get(download_url.url, params={'User-agent': f'your bot {uuid.uuid4()}'})\n",
    "            try:\n",
    "                logging.info(f\"Started writing downloaded data into json file: {download_url.file_path}\")\n",
    "                # saving downloaded data into hard disk\n",
    "                with open(download_url.file_path, \"w\") as file_obj:\n",
    "                    finance_complaint_data = list(map(lambda x: x[\"_source\"],\n",
    "                                                      filter(lambda x: \"_source\" in x.keys(),\n",
    "                                                             json.loads(data.content)))\n",
    "                                                  )\n",
    "\n",
    "                    json.dump(finance_complaint_data, file_obj)\n",
    "                logging.info(f\"Downloaded data has been written into file: {download_url.file_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.info(\"Failed to download hence retry again!\")\n",
    "                # removing file failed file exist\n",
    "                if os.path.exists(download_url.file_path):\n",
    "                    os.remove(download_url.file_path)\n",
    "                \n",
    "                self.retry_download_data(data, download_url=download_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    \n",
    "    def retry_download_data(self, data, download_url: DownloadURL):\n",
    "        try:\n",
    "            # if retry still possible try else return the response\n",
    "            if download_url.n_retry == 0:\n",
    "                self.failed_download_urls.append(download_url)\n",
    "                logging.info(f\"Unable to download file {download_url.url}\")\n",
    "                return\n",
    "            # to handle throatling requestion and can be slove if we wait for some second.\n",
    "            content = data.content.decode(\"utf-8\")\n",
    "            # The .findall() method iterates over a string to find a subset of characters that match a specified pattern.\n",
    "            wait_second = re.findall(r'\\d+', content)\n",
    "            \n",
    "            if len(wait_second) > 0:\n",
    "                time.sleep(int(wait_second[0]) + 2)\n",
    "\n",
    "            # Writing response to understand why request was failed\n",
    "            failed_file_path = os.path.join(self.config.failed_downloaded_dir, os.path.basename(download_url.file_path))\n",
    "            os.makedirs(self.config.failed_downloaded_dir, exist_ok=True)\n",
    "            with open(failed_file_path, \"wb\") as file_obj:\n",
    "                file_obj.write(data.content)\n",
    "\n",
    "             # calling download function again to retry\n",
    "            download_url = DownloadURL(download_url.url, file_path=download_url.file_path, n_retry=download_url.n_retry - 1)\n",
    "            self.download_data(download_url=download_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def convert_files_to_parquet(self, ) -> str:        \n",
    "        \"\"\"\n",
    "        downloaded files will be converted and merged into single parquet file\n",
    "        json_data_dir: downloaded json file directory\n",
    "        data_dir: converted and combined file will be generated in data_dir\n",
    "        output_file_name: output file name \n",
    "        =======================================================================================\n",
    "        returns output_file_path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            json_data_dir = self.config.downloaded_dir\n",
    "            data_dir = self.config.feature_store_dir\n",
    "            output_file_name = self.config.file_name\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            file_path = os.path.join(data_dir, f\"{output_file_name}\")\n",
    "            logging.info(f\"Parquet file will be created at: {file_path}\")\n",
    "            if not os.path.exists(json_data_dir):\n",
    "                return file_path\n",
    "            for file_name in os.listdir(json_data_dir):\n",
    "                json_file_path = os.path.join(json_data_dir, file_name)\n",
    "                logging.debug(f\"Converting {json_file_path} into parquet format at {file_path}\")\n",
    "                df = spark_session.read.json(json_file_path)\n",
    "                if df.count() > 0:\n",
    "                    df.write.mode('append').parquet(file_path)\n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def write_metadata(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        This function help us to update metadata information \n",
    "        so that we can avoid redundant download and merging.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Writing metadata info into metadata file.\")\n",
    "            metadata_info = DataIngestionMetadata(metadata_file_path=self.config.metadata_file_path)\n",
    "\n",
    "            metadata_info.write_metadata_info(from_date=self.config.from_date,\n",
    "                                              to_date=self.config.to_date,\n",
    "                                              data_file_path=file_path\n",
    "                                              )\n",
    "            logging.info(f\"Metadata has been written.\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "\n",
    "    def initiate_data_ingestion(self) -> DataIngestionConfig:\n",
    "        try:\n",
    "            logging.info(f\"Started downloading json file!\")\n",
    "            if self.config.from_date != self.config.to_date:\n",
    "                self.download_files()\n",
    "\n",
    "            if os.path.exists(self.config.downloaded_dir):\n",
    "                logging.info(f\"Converting and combining downloaded json into parquet file!\")\n",
    "                file_path = self.convert_files_to_parquet()\n",
    "                self.write_metadata(file_path=file_path)\n",
    "\n",
    "            feature_store_file_path = os.path.join(self.config.feature_store_dir,\n",
    "                                                   self.config.file_name)\n",
    "            artifact = DataIngestionArtifact(\n",
    "                feature_store_file_path=feature_store_file_path,\n",
    "                downloaded_dir=self.config.downloaded_dir,\n",
    "                metadata_file_path=self.config.metadata_file_path,\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Data ingestion artifact: {artifact}\")\n",
    "            return artifact\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.initiate_data_ingestion()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
